{
  "cells": [
    {
      "id": "19f20cbe-0cb4-498f-88b6-54e8673b37f3",
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# PDFドキュメント分析 - Snowflake Key Concepts\n\nSnowflake Cortex AI関数を使用してPDFドキュメントを分析します。\n\n**分析内容:**\n1. AI_PARSE_DOCUMENT - ドキュメント解析・画像抽出\n2. AI_EXTRACT - 機能名・キーワード抽出\n3. AI_AGG - ドキュメント要約\n4. AI_COMPLETE (Multimodal) - 抽出画像の分析\n5. SPLIT_TEXT_MARKDOWN_HEADER - チャンク分割\n\n**使用データ:** `@ai_discover.public.raw_stage/pdf/snowflake_key_concept.pdf`"
    },
    {
      "id": "3fd91ddc-9562-44e2-8e8e-64d9e40d5031",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_2",
        "language": "sql"
      },
      "source": "%%sql -r dataframe_2\nUSE SCHEMA ai_discover.public;",
      "outputs": [],
      "execution_count": 6
    },
    {
      "id": "a9b14829-0986-4c39-87cd-3f610aa7c60d",
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n## 1. AI_PARSE_DOCUMENT - ドキュメント解析・画像抽出\n\n`AI_PARSE_DOCUMENT`はPDF、Word、画像からテキスト・レイアウト・画像を抽出するCortex AI関数です。\n\n**特徴:**\n- PDF, DOCX, PPTX, 画像ファイルに対応\n- LAYOUTモードで表・ヘッダー構造を保持\n- `extract_images: true`で埋め込み画像を抽出\n\n**構文:**\n```sql\nAI_PARSE_DOCUMENT(\n    TO_FILE('@stage/document.pdf'),\n    {'mode': 'LAYOUT', 'extract_images': true}\n)\n```"
    },
    {
      "id": "0c316f9b-442f-43ec-80b7-28b332c0c168",
      "cell_type": "code",
      "metadata": {
        "language": "sql",
        "resultVariableName": "parse_result",
        "title": "AI_PARSE_DOCUMENT実行"
      },
      "source": "%%sql -r parse_result\nCREATE OR REPLACE TABLE DOC_PARSED AS\nSELECT \n    'snowflake_key_concept.pdf' AS document_name,\n    AI_PARSE_DOCUMENT(\n        TO_FILE('@ai_discover.public.raw_stage/pdf/snowflake_key_concept.pdf'),\n        {'mode': 'LAYOUT', 'extract_images': true, 'page_split': true}\n    ) AS parsed_content",
      "outputs": [],
      "execution_count": 7
    },
    {
      "id": "907eb49e-24de-4525-8e1f-5e4b5db042aa",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_1",
        "language": "sql"
      },
      "source": "%%sql -r dataframe_1\nSELECT PARSED_CONTENT:pages[0].content as content FROM DOC_PARSED LIMIT 100;",
      "outputs": [],
      "execution_count": 21
    },
    {
      "id": "8c42de8b-c1c8-4515-b24d-59094e811dc2",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "title": "解析結果表示",
        "collapsed": false,
        "codeCollapsed": false
      },
      "source": "from snowflake.snowpark.context import get_active_session\nimport json\nimport re\nsession = get_active_session()\ndf = session.sql('SELECT PARSED_CONTENT FROM DOC_PARSED').to_pandas()\nparsed = df['PARSED_CONTENT'].iloc[0]\nif isinstance(parsed, str):\n    parsed = json.loads(parsed)\npage_count = len(parsed.get('pages', []))\ntotal_images = sum(len(page.get('images', [])) for page in parsed.get('pages', []))\nprint(f'ページ数: {page_count}')\nprint(f'抽出画像数: {total_images}')\n\nfull_text = '\\n\\n'.join(page.get('content', '') for page in parsed.get('pages', []))\nsections = re.split(r'(^#{1,2}\\s+.+$)', full_text, flags=re.MULTILINE)\nprint('\\n=== 章・節構成（先頭3件）===')\ncount = 0\nfor i, section in enumerate(sections):\n    if re.match(r'^#{1,2}\\s+', section):\n        level = '章' if section.startswith('# ') else '節'\n        title = section.strip()\n        content = sections[i+1].strip()[:200] if i+1 < len(sections) else ''\n        print(f'\\n【{level}】{title}')\n        print(f'{content}...')\n        count += 1\n        if count >= 3:\n            break",
      "outputs": [],
      "execution_count": 4
    },
    {
      "id": "3e6decf7-708e-4f1f-9ab7-fca550137584",
      "cell_type": "code",
      "metadata": {
        "language": "sql",
        "resultVariableName": "save_images_proc",
        "title": "画像保存プロシージャ作成"
      },
      "source": "%%sql -r save_images_proc\nCREATE OR REPLACE PROCEDURE SAVE_EXTRACTED_IMAGES(r VARIANT)\nRETURNS ARRAY\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.9'\nPACKAGES = ('pillow', 'snowflake-snowpark-python')\nHANDLER = 'run'\nAS\n$$\nimport base64\nimport io\nimport os\nimport tempfile\nfrom PIL import Image\n\ndef process_parse_document_result(data: dict) -> tuple[str, str, str]:\n    for page in data.get(\"pages\", []):\n        for image in page.get(\"images\", []):\n            id = image[\"id\"]\n            data_str, image_base64 = image[\"image_base64\"].split(\";\", 1)\n            extension = data_str.split(\"/\")[1]\n            base64_data = image_base64.split(\",\")[1]\n            yield id, extension, base64_data\n\ndef decode_base64(encoded_image: str) -> bytes:\n    return base64.b64decode(encoded_image)\n\ndef run(session, r):\n    destination_path = r[\"DESTINATION_PATH\"]\n    parse_document_result = r[\"PARSE_DOCUMENT_RESULT\"]\n    if not destination_path or not destination_path.startswith(\"@\"):\n        return [\"Error: destination_path must start with @\"]\n    uploaded_files = []\n    with tempfile.TemporaryDirectory() as temp_dir:\n        for image_id, extension, encoded_image in process_parse_document_result(parse_document_result):\n            image_bytes = decode_base64(encoded_image)\n            image: Image = Image.open(io.BytesIO(image_bytes))\n            image_path = os.path.join(temp_dir, image_id)\n            image.save(image_path)\n            session.file.put(image_path, destination_path, auto_compress=False, overwrite=True)\n            uploaded_files.append(f\"{destination_path}/{image_id}\")\n            os.remove(image_path)\n    return uploaded_files\n$$",
      "outputs": [],
      "execution_count": 6
    },
    {
      "id": "636f11a7-2aba-4f18-ac70-b9a6c7874731",
      "cell_type": "code",
      "metadata": {
        "language": "sql",
        "resultVariableName": "save_images_result",
        "title": "画像をステージに保存"
      },
      "source": "%%sql -r save_images_result\nCALL SAVE_EXTRACTED_IMAGES(\n    (SELECT OBJECT_CONSTRUCT(*)\n     FROM (SELECT\n         '@ai_discover.public.images' AS destination_path,\n         PARSED_CONTENT AS parse_document_result\n     FROM DOC_PARSED) LIMIT 1)\n)",
      "outputs": [],
      "execution_count": 8
    },
    {
      "id": "086a0054-7a60-4f74-949c-873ec1b213d8",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "title": "保存画像一覧表示"
      },
      "source": "import tempfile\nimport os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ndf = session.sql(\"SELECT * FROM DIRECTORY('@ai_discover.public.images')\").to_pandas()\nprint(f'保存された画像数: {len(df)}')\nfor idx, row in df.iterrows():\n    print(f\"  - {row['RELATIVE_PATH']}\")\n\nwith tempfile.TemporaryDirectory() as temp_dir:\n    session.file.get('@ai_discover.public.images/img-0.jpeg', temp_dir)\n    img_path = os.path.join(temp_dir, 'img-0.jpeg')\n    img = Image.open(img_path)\n    plt.figure(figsize=(10, 8))\n    plt.imshow(img)\n    plt.axis('off')\n    plt.title('img-0.jpeg')\n    plt.show()",
      "outputs": [],
      "execution_count": 9
    },
    {
      "id": "4fc94abd-9e07-45e9-ac10-628a38bd31f0",
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n## 2. AI_EXTRACT - 機能名・キーワード抽出\n\n`AI_EXTRACT`は非構造化データから特定の情報を抽出するCortex AI関数です。\n\n**特徴:**\n- 質問形式で抽出内容を指定\n- 複数の項目を一度に抽出可能\n- 画像・テキストの両方に対応\n\n**構文:**\n```sql\nAI_EXTRACT(text, {'feature_names': 'List all feature names mentioned'})\n```"
    },
    {
      "id": "815c677b-36cc-4091-b516-9c486660cd5a",
      "cell_type": "code",
      "metadata": {
        "language": "sql",
        "resultVariableName": "extract_result",
        "title": "AI_EXTRACT実行"
      },
      "source": "%%sql -r extract_result\nCREATE OR REPLACE TABLE DOC_EXTRACTED AS\nWITH full_text AS (\n    SELECT LISTAGG(p.value:content::STRING, '\\n\\n') AS doc_text\n    FROM DOC_PARSED, LATERAL FLATTEN(input => PARSED_CONTENT:pages) p\n)\nSELECT \n    doc_text,\n    AI_EXTRACT(\n        doc_text,\n        {\n            'feature_names': 'ドキュメント内で言及されているSnowflakeの機能名や製品名をすべてリストアップしてください（例: Snowpipe, Streams, Tasks等）',\n            'key_concepts': 'このドキュメントで説明されている主要な技術コンセプトをリストアップしてください',\n            'use_cases': '記載されている主なユースケースやメリットは何ですか？'\n        }\n    ) AS extracted_info\nFROM full_text",
      "outputs": [],
      "execution_count": 10
    },
    {
      "id": "40b23240-6b4a-444d-bb18-cfb78e0af80e",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "title": "抽出結果表示"
      },
      "source": "df = session.sql('SELECT EXTRACTED_INFO FROM DOC_EXTRACTED').to_pandas()\nextracted = df['EXTRACTED_INFO'].iloc[0]\nif isinstance(extracted, str):\n    extracted = json.loads(extracted)\nresponse = extracted.get('response', extracted)\nprint('=== 抽出結果 ===')\nfor key, value in response.items():\n    print(f'\\n【{key}】')\n    if isinstance(value, list):\n        for item in value:\n            print(f'  - {item}')\n    else:\n        print(f'  {value}')",
      "outputs": [],
      "execution_count": 11
    },
    {
      "id": "a3f18268-bb6c-45f0-a039-f5e4f93135d1",
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n## 3. AI_AGG - ドキュメント要約\n\n`AI_AGG`は複数行のテキストを集約・要約するCortex AI関数です。\n\n**特徴:**\n- GROUP BYと組み合わせて複数レコードを要約\n- カスタムプロンプトで要約形式を指定\n- 長文を指定した文字数に圧縮\n\n**構文:**\n```sql\nAI_AGG(text_column, 'Summarize in 300 words') ... GROUP BY ...\n```"
    },
    {
      "id": "cac1f77e-bddf-4ee7-ace9-4dee2b2a715c",
      "cell_type": "code",
      "metadata": {
        "language": "sql",
        "resultVariableName": "agg_result",
        "title": "AI_AGG実行"
      },
      "source": "%%sql -r agg_result\nCREATE OR REPLACE TABLE DOC_SUMMARY AS\nSELECT \n    AI_AGG(\n        p.value:content::STRING, \n        'Summarize the key points of this Snowflake documentation in 300 words. \n         Focus on the main features, benefits, and technical concepts. \n         Require that output format is JSON.'\n    ) AS summary\nFROM DOC_PARSED, LATERAL FLATTEN(input => PARSED_CONTENT:pages) p\nGROUP BY 1=1",
      "outputs": [],
      "execution_count": 12
    },
    {
      "id": "dba3f4a5-268d-40b8-a668-fa0cf87e2ee4",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "title": "要約結果表示"
      },
      "source": "df = session.sql('SELECT SUMMARY FROM DOC_SUMMARY').to_pandas()\nprint('=== ドキュメント要約 ===')\nprint(df['SUMMARY'].iloc[0])",
      "outputs": [],
      "execution_count": 13
    },
    {
      "id": "4dc5f3ba-de50-45a5-a44e-cb1c7a82a30e",
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n## 4. AI_COMPLETE (Multimodal) - 抽出画像の分析\n\n`AI_COMPLETE`はマルチモーダル対応モデルで画像を分析できます。\n\n**特徴:**\n- pixtral-large等のマルチモーダルモデルを使用\n- ステージ上の画像ファイルを直接分析\n- 画像内のテキスト・グラフ・図表を理解\n\n**構文:**\n```sql\nAI_COMPLETE(\n    'pixtral-large',\n    'Describe this image',\n    TO_FILE('@stage/image.jpeg')\n)\n```"
    },
    {
      "id": "9c666215-bd27-479e-a8e8-ebe667314869",
      "cell_type": "code",
      "metadata": {
        "language": "sql",
        "resultVariableName": "image_analysis_result",
        "title": "AI_COMPLETE（画像分析）実行"
      },
      "source": "%%sql -r image_analysis_result\nCREATE OR REPLACE TABLE DOC_IMAGE_ANALYSIS AS\nSELECT \n    RELATIVE_PATH AS image_file,\n    AI_COMPLETE(\n        'claude-4-sonnet',\n        'この画像を日本語で簡潔に説明してください。図表の場合は、何を表しているかを説明してください。',\n        TO_FILE('@ai_discover.public.images/' || RELATIVE_PATH)\n    ) AS image_description\nFROM DIRECTORY('@ai_discover.public.images')\nWHERE RELATIVE_PATH LIKE '%.jpeg' OR RELATIVE_PATH LIKE '%.png'\nLIMIT 5",
      "outputs": [],
      "execution_count": 14
    },
    {
      "id": "d424b70c-70ca-4a64-bd98-11e9af259076",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "title": "画像分析結果表示"
      },
      "source": "df = session.sql('SELECT * FROM DOC_IMAGE_ANALYSIS').to_pandas()\nprint('=== 画像分析結果 ===')\nfor idx, row in df.iterrows():\n    print(f\"\\n【{row['IMAGE_FILE']}】\")\n    desc = row['IMAGE_DESCRIPTION']\n    if isinstance(desc, str):\n        desc = json.loads(desc)\n    if isinstance(desc, dict):\n        response = desc.get('response', desc)\n        if isinstance(response, dict):\n            for key, value in response.items():\n                print(f'  [{key}]')\n                if isinstance(value, list):\n                    for item in value:\n                        print(f'    - {item}')\n                else:\n                    print(f'    {value}')\n        else:\n            print(f'  {response}')\n    else:\n        print(f'  {desc}')",
      "outputs": [],
      "execution_count": 15
    },
    {
      "id": "d2a7fe4c-18fe-413a-9a05-e1dd233eebd5",
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n## 5. SPLIT_TEXT_MARKDOWN_HEADER - チャンク分割\n\n`SPLIT_TEXT_MARKDOWN_HEADER`はMarkdownヘッダーベースでテキストをチャンク分割するCortex AI関数です。\n\n**特徴:**\n- ヘッダー階層（#, ##, ###）を認識\n- 各チャンクにヘッダー情報を付与\n- チャンクサイズとオーバーラップを指定可能\n- RAGパイプラインに最適\n\n**構文:**\n```sql\nSPLIT_TEXT_MARKDOWN_HEADER(\n    text,\n    {'#': 'header_1', '##': 'header_2'},\n    chunk_size,\n    overlap\n)\n```"
    },
    {
      "id": "10b014b6-e7ce-4482-8ae7-b45e619f2561",
      "cell_type": "code",
      "metadata": {
        "language": "sql",
        "resultVariableName": "chunk_result",
        "title": "SPLIT_TEXT_MARKDOWN_HEADER実行"
      },
      "source": "%%sql -r chunk_result\nCREATE OR REPLACE TABLE DOC_CHUNKS AS\nWITH full_text AS (\n    SELECT LISTAGG(p.value:content::STRING, '\\n\\n') AS doc_text\n    FROM DOC_PARSED, LATERAL FLATTEN(input => PARSED_CONTENT:pages) p\n)\nSELECT \n    c.index AS chunk_index,\n    c.value:chunk::STRING AS chunk_text,\n    c.value:headers::OBJECT AS chunk_headers,\n    LENGTH(c.value:chunk::STRING) AS chunk_length\nFROM full_text,\nLATERAL FLATTEN(\n    SNOWFLAKE.CORTEX.SPLIT_TEXT_MARKDOWN_HEADER(\n        doc_text,\n        OBJECT_CONSTRUCT('#', 'header_1', '##', 'header_2', '###', 'header_3'),\n        1000,\n        100\n    )\n) c",
      "outputs": [],
      "execution_count": 16
    },
    {
      "id": "d07af5b0-1651-4872-ac10-4a60cb6722a1",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.context import get_active_session\nimport json\nimport re\nsession = get_active_session()\ndf = session.sql('SELECT PARSED_CONTENT FROM DOC_PARSED').to_pandas()\nparsed = df['PARSED_CONTENT'].iloc[0]\nif isinstance(parsed, str):\n    parsed = json.loads(parsed)\npage_count = len(parsed.get('pages', []))\ntotal_images = sum(len(page.get('images', [])) for page in parsed.get('pages', []))\nprint(f'ページ数: {page_count}')\nprint(f'抽出画像数: {total_images}')\n\nfull_text = '\\n\\n'.join(page.get('content', '') for page in parsed.get('pages', []))\nsections = re.split(r'(^#{1,2}\\s+.+$)', full_text, flags=re.MULTILINE)\nprint('\\n=== 章・節構成（先頭3件）===')\ncount = 0\nfor i, section in enumerate(sections):\n    if re.match(r'^#{1,2}\\s+', section):\n        level = '章' if section.startswith('# ') else '節'\n        title = section.strip()\n        content = sections[i+1].strip()[:200] if i+1 < len(sections) else ''\n        print(f'\\n【{level}】{title}')\n        print(f'{content}...')\n        count += 1\n        if count >= 3:\n            break",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f4080030-a30f-41a5-9d88-bc5d55a62948",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c9bcf18e-6d09-4e26-81e9-85951961e037",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "title": "チャンク結果表示"
      },
      "source": "df = session.sql('SELECT * FROM DOC_CHUNKS ORDER BY CHUNK_INDEX').to_pandas()\nprint(f'=== チャンク統計 ===')\nprint(f'総チャンク数: {len(df)}')\nprint(f'平均チャンク長: {df[\"CHUNK_LENGTH\"].mean():.0f}文字')\nprint(f'最小チャンク長: {df[\"CHUNK_LENGTH\"].min()}文字')\nprint(f'最大チャンク長: {df[\"CHUNK_LENGTH\"].max()}文字')\nprint(f'\\n=== チャンクサンプル（最初の3件）===')\nfor idx, row in df.head(3).iterrows():\n    headers = row['CHUNK_HEADERS']\n    if isinstance(headers, str):\n        headers = json.loads(headers)\n    print(f\"\\n【チャンク {row['CHUNK_INDEX']}】\")\n    print(f\"ヘッダー: {headers}\")\n    print(f\"テキスト: {row['CHUNK_TEXT'][:200]}...\")",
      "outputs": [],
      "execution_count": 17
    },
    {
      "id": "f1a2b3c4-d5e6-7890-abcd-ef1234567890",
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n## まとめ\n\n### 使用したCortex AI関数\n\n| 関数 | 用途 | 出力 |\n|------|------|------|\n| `AI_PARSE_DOCUMENT` | ドキュメント解析・画像抽出 | Markdown + 画像データ |\n| `AI_EXTRACT` | キーワード・情報抽出 | 構造化データ |\n| `AI_AGG` | テキスト要約 | 要約テキスト |\n| `AI_COMPLETE` (Multimodal) | 画像分析 | 画像説明テキスト |\n| `SPLIT_TEXT_MARKDOWN_HEADER` | チャンク分割 | チャンク配列 |\n\n### 分析フロー\n\n```\nPDFファイル\n    ↓ AI_PARSE_DOCUMENT (LAYOUT + extract_images)\nMarkdownテキスト + 画像\n    ├─→ AI_EXTRACT → 機能名・キーワード\n    ├─→ AI_AGG → 要約\n    ├─→ SPLIT_TEXT_MARKDOWN_HEADER → チャンク\n    └─→ ステージ保存 → AI_COMPLETE (Multimodal) → 画像分析\n```\n\n### ポイント\n\n1. **画像抽出**: `extract_images: true`オプションで埋め込み画像をBase64で取得\n2. **ステージ保存**: Pythonプロシージャで画像をデコードしステージに保存\n3. **マルチモーダル分析**: `pixtral-large`モデルでステージ上の画像を直接分析\n4. **チャンク分割**: Markdownヘッダー構造を活用した意味のあるチャンク生成\n5. **RAG対応**: チャンクにヘッダー情報を付与することで検索精度向上"
    },
    {
      "id": "a1b2c3d4-e5f6-7890-1234-567890abcdef",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "title": "分析サマリー"
      },
      "source": "print('=' * 60)\nprint('ドキュメント分析完了')\nprint('=' * 60)\ndf_parsed = session.sql('SELECT PARSED_CONTENT FROM DOC_PARSED').to_pandas()\nparsed = df_parsed['PARSED_CONTENT'].iloc[0]\nif isinstance(parsed, str):\n    parsed = json.loads(parsed)\ndf_chunks = session.sql('SELECT COUNT(*) AS cnt FROM DOC_CHUNKS').to_pandas()\ndf_images = session.sql(\"SELECT COUNT(*) AS cnt FROM DIRECTORY('@ai_discover.public.images')\").to_pandas()\nprint(f\"\\nドキュメント: snowflake_key_concept.pdf\")\nprint(f\"ページ数: {parsed.get('metadata', {}).get('pageCount', 0)}\")\nprint(f\"抽出画像数: {df_images['CNT'].iloc[0]}\")\nprint(f\"生成チャンク数: {df_chunks['CNT'].iloc[0]}\")\nprint(f\"\\n生成テーブル:\")\nprint(\"  - DOC_PARSED (解析結果)\")\nprint(\"  - DOC_EXTRACTED (抽出情報)\")\nprint(\"  - DOC_SUMMARY (要約)\")\nprint(\"  - DOC_IMAGE_ANALYSIS (画像分析)\")\nprint(\"  - DOC_CHUNKS (チャンク)\")\nprint(f\"\\n生成ステージ:\")\nprint(\"  - @ai_discover.public.images (抽出画像)\")",
      "outputs": [],
      "execution_count": 17
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}